{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198fafb9-477a-4603-a10f-6dcabcf9d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94793c-4e49-4233-a76c-48737fd64cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, path_to_data, ratio_train):\n",
    "        with open(path_to_data, mode='r', encoding='utf-8', errors='replace') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        self.content = content\n",
    "        \n",
    "        # (1) using Home-made solution\n",
    "        # self.vocab = sorted(list(set(content)))\n",
    "        # self.n_vocab = len(self.vocab)\n",
    "        # dict_ctoi = { char:idx for idx, char in enumerate(self.vocab) }\n",
    "        # dict_itoc = { idx:char for idx, char in enumerate(self.vocab) }\n",
    "        # self.fn_encode = lambda s: [dict_ctoi[c] for c in s]\n",
    "        # self.fn_decode = lambda s: ''.join([dict_itoc[i] for i in s])\n",
    "\n",
    "        # (2) using tiktoken\n",
    "        encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.n_vocab = encoding.n_vocab\n",
    "        self.fn_encode = encoding.encode\n",
    "        self.fn_decode = encoding.decode\n",
    "\n",
    "        data = torch.tensor(self.fn_encode(content), dtype=torch.long)\n",
    "        n = int(len(data) * ratio_train)\n",
    "        self.train_data = data[:n]\n",
    "        self.eval_data = data[n:]\n",
    "\n",
    "    def collate_data(self, category, batch_size, context_size):\n",
    "        data = self.train_data if category == 'train' else self.eval_data\n",
    "        batch_start_idx = torch.randint(len(data) - context_size - 1, (batch_size,))\n",
    "        x = torch.stack([data[idx:idx+context_size] for idx in batch_start_idx])\n",
    "        y = torch.stack([data[idx+1:idx+context_size+1] for idx in batch_start_idx])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e33e3fc-f010-4041-a739-5a7e313ee526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in:  3712783\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "# dc = DataCollator('./TinyS.txt', 0.9)\n",
    "dc = DataCollator('./Tolkien.txt', 0.9)\n",
    "\n",
    "print(\"Read in: \", len(dc.content))\n",
    "print(dc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0130ee-ac7a-4dba-91fb-1872419ab4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, head_size, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.key = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, c, f)\n",
    "        batch, ctx, features = x.shape\n",
    "        # q or k: (b, c, f) @ (f, h) = (b, c, h) where h(head_size) = f / n_head\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        # calc attention score, w: (b, c, c)\n",
    "        w = q @ k.transpose(-2, -1) * q.shape[-1]**-0.5\n",
    "        w = w.masked_fill(self.tril[:ctx, :ctx] == 0, float('-inf'))\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        # cal weighted value, v: (b, c, h)\n",
    "        v = self.value(x)\n",
    "        # (b, c, c) @ (b, c, h) = (b, c ,h)\n",
    "        rslt = w @ v\n",
    "        return rslt\n",
    "\n",
    "# params: 4 * n_embedding ^ 2 (Q, K, V, projection)\n",
    "class MaskedMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head\n",
    "        self.heads = torch.nn.ModuleList([MaskedSingleHeadAttention(head_size, context_size, n_embedding, dropout_p) for _ in range(n_head)])\n",
    "        self.projection = torch.nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, c ,h) --cat--> (b, c, f)\n",
    "        rslt = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        rslt = self.dropout(self.projection(rslt))\n",
    "        return rslt\n",
    "\n",
    "# params: 2 * 4 * n_embedding ^ 2\n",
    "class FeedFoward(torch.nn.Module):\n",
    "    def __init__(self, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embedding, n_embedding * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_embedding * 4, n_embedding),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class TransformerUnit(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.mha = MaskedMultiHeadAttention(n_head, context_size, n_embedding, dropout_p)\n",
    "        self.ff = FeedFoward(n_embedding, dropout_p)\n",
    "        self.mha_ln = torch.nn.LayerNorm(n_embedding)\n",
    "        self.ff_ln = torch.nn.LayerNorm(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.mha_ln(x))\n",
    "        x = x + self.ff(self.ff_ln(x))\n",
    "        return x\n",
    "\n",
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding\n",
    "class NaiveLangModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_head, context_size, n_embedding, dropout_p, use_tie):\n",
    "        super().__init__()\n",
    "        # params: vocab_size * n_embedding\n",
    "        self.token_embed = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        # params: context_size * n_embedding\n",
    "        self.position_embed = torch.nn.Embedding(context_size, n_embedding)\n",
    "        self.units = torch.nn.Sequential(*[TransformerUnit(n_head, context_size, n_embedding, dropout_p) for _ in range(n_layer)])\n",
    "        self.ln = torch.nn.LayerNorm(n_embedding)\n",
    "        if use_tie:\n",
    "            self.lm_head = torch.nn.Linear(n_embedding, vocab_size, bias=False)\n",
    "            self.lm_head.weight = self.token_embed.weight\n",
    "        else:\n",
    "            # params: vocab_size * n_embedding\n",
    "            self.lm_head = torch.nn.Linear(n_embedding, vocab_size)\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.use_tie = use_tie\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch, ctx = inputs.shape\n",
    "        # t_embed: (b, c, f); p_embed: (c,f)\n",
    "        t_embed = self.token_embed(inputs)\n",
    "        p_embed = self.position_embed(torch.arange(ctx, device=device))\n",
    "        # x: (b, c, f)\n",
    "        x = t_embed + p_embed\n",
    "        x = self.units(x)\n",
    "        x = self.ln(x)\n",
    "        # logits: (b, c, v) \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch, ctx, features = logits.shape\n",
    "        predicts = logits.view(batch*ctx, features)\n",
    "        targets = labels.view(batch*ctx)\n",
    "        return logits, torch.nn.functional.cross_entropy(predicts, targets)\n",
    "\n",
    "    def generate(self, inputs, max_gen):\n",
    "        for _ in range(max_gen):\n",
    "            inputs_last_window = inputs[:, -self.context_size:]\n",
    "            logits, loss = self(inputs_last_window)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_next = torch.multinomial(torch.nn.functional.softmax(logits, dim=1), num_samples=1)\n",
    "            inputs = torch.cat((inputs, pred_next), dim=1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961fdd3f-8e0d-4501-8a18-45a9e4e28915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.451456 M parameters\n"
     ]
    }
   ],
   "source": [
    "n_layer = 4\n",
    "#n_head = 4\n",
    "n_head = 6\n",
    "# n_embedding = 256\n",
    "n_embedding = 192\n",
    "dropout_p = 0.2\n",
    "context_size=128 # context length for prediction\n",
    "\n",
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding + 12 * n_embedding ^ 2\n",
    "model = NaiveLangModel(vocab_size=dc.n_vocab, n_layer=n_layer, n_head=n_head, context_size=context_size, n_embedding=n_embedding, dropout_p=dropout_p, use_tie=True)\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6f712-a91a-4a51-8991-eeb9d81133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20729c9-0b73-4250-8fa2-29828f2ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, batch_size, steps, eval_interval, n_eval, weight_decay):\n",
    "    @torch.no_grad()\n",
    "    def calc_loss(n_eval, batch_size):\n",
    "        rslt = {}\n",
    "        model.eval()\n",
    "        for c in ['train', 'eval']:\n",
    "            losses = torch.zeros(n_eval)\n",
    "            for i in range(n_eval):\n",
    "                x, y = dc.collate_data(c, batch_size, model.context_size)\n",
    "                _, loss = model(x, y)\n",
    "                losses[i] = loss.item()\n",
    "            rslt[c] = losses.mean()\n",
    "        model.train()\n",
    "        return rslt\n",
    "\n",
    "    def prepare_optimizer(learning_rate, weight_decay):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            for pname, param in module.named_parameters(recurse=False):\n",
    "                full_name = f\"{name}.{pname}\" if name else pname\n",
    "                if pname.endswith(\"bias\") or \"ln\" in name or \"ln\" in pname or \"embed\" in name or \"lm_head\" in name:\n",
    "                    no_decay.add(full_name)\n",
    "                else:\n",
    "                    decay.add(full_name)\n",
    "\n",
    "        if model.use_tie:\n",
    "            no_decay.remove(\"lm_head.weight\")\n",
    "\n",
    "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    # Apply weight decay\n",
    "    optimizer = prepare_optimizer(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    for step in range(steps):\n",
    "        if step % eval_interval == 0 or step == steps - 1:\n",
    "            losses = calc_loss(n_eval, batch_size)\n",
    "            print(f\"[step {step}] train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "        x, y = dc.collate_data('train', batch_size, model.context_size)\n",
    "        _, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae67b13a-cd48-4cf0-9cb1-6c409b969a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] train loss 121.8590, eval loss 126.5154\n",
      "[step 100] train loss 12.5827, eval loss 12.9138\n",
      "[step 200] train loss 7.4084, eval loss 8.0077\n",
      "[step 300] train loss 6.5423, eval loss 7.0730\n",
      "[step 400] train loss 6.2756, eval loss 6.7553\n",
      "[step 500] train loss 6.0633, eval loss 6.5518\n",
      "[step 600] train loss 5.9132, eval loss 6.3395\n",
      "[step 700] train loss 5.7668, eval loss 6.1805\n",
      "[step 800] train loss 5.6994, eval loss 6.0842\n",
      "[step 900] train loss 5.5979, eval loss 6.0137\n",
      "[step 1000] train loss 5.5396, eval loss 5.9779\n",
      "[step 1100] train loss 5.4641, eval loss 5.9002\n",
      "[step 1200] train loss 5.4007, eval loss 5.8494\n",
      "[step 1300] train loss 5.3256, eval loss 5.7650\n",
      "[step 1400] train loss 5.2803, eval loss 5.7374\n",
      "[step 1500] train loss 5.2184, eval loss 5.6691\n",
      "[step 1600] train loss 5.1618, eval loss 5.6234\n",
      "[step 1700] train loss 5.0962, eval loss 5.5768\n",
      "[step 1800] train loss 5.0295, eval loss 5.5027\n",
      "[step 1900] train loss 4.9752, eval loss 5.4474\n",
      "[step 2000] train loss 4.9292, eval loss 5.4092\n",
      "[step 2100] train loss 4.8779, eval loss 5.3458\n",
      "[step 2200] train loss 4.8124, eval loss 5.2901\n",
      "[step 2300] train loss 4.7744, eval loss 5.2423\n",
      "[step 2400] train loss 4.7278, eval loss 5.1977\n",
      "[step 2500] train loss 4.6845, eval loss 5.1417\n",
      "[step 2600] train loss 4.6410, eval loss 5.0855\n",
      "[step 2700] train loss 4.5800, eval loss 5.0403\n",
      "[step 2800] train loss 4.5312, eval loss 5.0039\n",
      "[step 2900] train loss 4.5145, eval loss 4.9799\n",
      "[step 3000] train loss 4.4567, eval loss 4.9358\n",
      "[step 3100] train loss 4.4338, eval loss 4.8921\n",
      "[step 3200] train loss 4.3859, eval loss 4.8596\n",
      "[step 3300] train loss 4.3533, eval loss 4.8351\n",
      "[step 3400] train loss 4.3348, eval loss 4.7982\n",
      "[step 3500] train loss 4.2889, eval loss 4.7733\n",
      "[step 3600] train loss 4.2674, eval loss 4.7638\n",
      "[step 3700] train loss 4.2474, eval loss 4.7296\n",
      "[step 3800] train loss 4.2250, eval loss 4.7073\n",
      "[step 3900] train loss 4.1905, eval loss 4.7045\n",
      "[step 4000] train loss 4.1575, eval loss 4.6790\n",
      "[step 4100] train loss 4.1557, eval loss 4.6704\n",
      "[step 4200] train loss 4.0995, eval loss 4.6482\n",
      "[step 4300] train loss 4.0912, eval loss 4.6155\n",
      "[step 4400] train loss 4.0700, eval loss 4.6019\n",
      "[step 4500] train loss 4.0468, eval loss 4.5874\n",
      "[step 4600] train loss 4.0211, eval loss 4.5864\n",
      "[step 4700] train loss 4.0105, eval loss 4.5664\n",
      "[step 4800] train loss 3.9850, eval loss 4.5334\n",
      "[step 4900] train loss 3.9666, eval loss 4.5390\n",
      "[step 4999] train loss 3.9454, eval loss 4.5055\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "eval_interval = 100 # evaluate every N steps\n",
    "# batch_size = 128\n",
    "batch_size = 64\n",
    "n_eval = 100       # evaluate n_eval times then calculate the mean\n",
    "#lr = 3e-4\n",
    "lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "\n",
    "train_model(learning_rate=lr, batch_size=batch_size, steps=steps, eval_interval=eval_interval, n_eval=n_eval, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3db2b7-82cf-44fe-b47a-91774bd71887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! And he he grabbed it would think Gollum were he wanted as he was going on, on his spears to the king.\n",
      "     Faramir and Pippin looked at him. ' muttered only before he trotiness to the fall he tossed his little, at last he rose and went beside him till Gandalf's mouth cominged at that strange thing peered from Anguimertseven.  o'marum: most of a little dawn were ordered a whip looked it down, and he gave on his openly his fingers drew his village. 'I release him with its Frodo could not make, he too. He was more than Sam!'\n",
      "     '\" Grishn kh!' said Ugl k. \"Here there was a piring with that Precious we path more than the way back by long ago-th, for Finwepered, and indeed he had no time wh rhouse. But he ask riding on this cold words that he dwarves are his light, or end is to the bottom; for shall most of some time and one never knew no good, if he wish he is creeping on a mighty gates of Osgiliids, and he calls what of often been my friend, but his friends. For a moment outlandish entered, and he vanished into the globe. He could wish; he slipped out of the night; for he is gone up into the autumn.\n",
      "     Now he led him. 'I may read he was very armed now the land went to the walls of stout at Sauron among the Beginning! I guess! ' he said became supplemented and restrain his birthday. Then he he had a long chair. He, and he laughed.\n",
      "     It was all he had to overthrow. 'That may be being from your way, and these things that he'd take three such small man, and it's going wrong. He bit to any, and the quest. I a air!' He sank at once at once in which he do.\n",
      "     'We must be broken, Mr. Butterbur's a look on and the outer wall, and one of the White Hand, was clothed both down our enemy at least so miles wide that's kept the chamber under the log of the sun. Gollum peering with possible open beasts were Shagrat, and pull again, and departed with Burg. The path sat and there was furlongs, and gently down a black door. It was alreadyushing to him to mischief; but ever there came in was the man's forehead with with in his spear sprang bush and had thin round him that dark laughter was now sail.\n",
      "    `If you put me away to speak a former breath. What once that was all some speaks-not, and a great sad-cleurt. What do he?'\n",
      "     `cat about about him. 'Sureo amusing here alone. 'No's no longer in the tubs moved. None, he'll northern men!' cried Gandalf opened his breast the pocket, step upon the sword.\n",
      "\n",
      "     At this is glanced his stagger leg. For a while he would have, and Bree quite more sense on the bottom before he could we not hold them. His captain, but the east Company without wounded: birds was not quite levhed you, coming, if you'll am without the passage.'\n",
      "     `Gandalf, run small morning,' he said.\n",
      "     This sleep there were nothing against the note, and bowing voice out of Emyn Muil and Galadrielongue. Far out of the where will they have had ridden to go so, here.'\n",
      "     When she Imrahil gazed at her. 'But I remember, Grgollum doubted, thou now and no tale of robe for many holes, leaving the duty and vain at the point   omer's more arrival this Council. A growing lands. No thing ripe-diddle in Moria. _FALL_ lo!_\n",
      "     I shoot the last grievously.\n",
      "     `I would not have my do not tell thou longer as for a ring. 'While I should like a tale! Not it, your loop is a feast; and far but trying to assail me and the Moon came from the gate.\n",
      "     'It seems for you, and your side. See well! He does never seen; and Yellow Face?'\n",
      "     `Boorn did not drive children of you and Gollum! Fair, is way-hunter indeed advice!'\n",
      "     You comes     He listened his thought to his boat. `I must go. But nothing's rouse me. I said to me? The Master hobbits kindly so. You will not come back with them and still now for him the ships.' People turned and put merely closely.Melkings of his knees as he spoke to Elrond, that evil is boldness was free. These are is on. Tom would found in the box and leave our foul gate. So then sighed, that he is old and all up his mouth. And so he's my legs called by stairs, in his chief villain eyes at the look of the last upon the dark moment they had brought into the heal Realm for the land. Some should he think there a while. If the West travelledness they will be done in the night; and irresistise and more certain hundreds of the comrades but it will take Araman was that is a good great fear of gift of Doom. With guilt live paley, going to the inquullos of the mound for their hearts of the fearnach. For it is over new eaves him the dread-race she, and a Whitewark, save them that is the last need; for that terrible he do not need after the burning men. The Whiteskins that I do not offered them in the land, bound Valley of the king, their \n",
      " as huge in the slain and very colder far away who most left that he said. \n",
      "Of now, and at last, it seemed like a pit of strength more off MILhhollow before D neness with the \n",
      "long barrel, who had told even the people of Sleep or the southward, and their spaces quays stayedmaking. Then Voraries themselves with \n",
      "are preid, though she might Felageking and daylight and King; and her to their own kin: \n",
      "\n",
      "\n",
      "be spears lands the gloat were wide with the left Men.-deriand with a Sale fires- \n",
      "his purpose. But after Buck-king’s prisoners, and It flashed on the field; but not for every chance of \n",
      "of the Rangers were, and remembered as they were on a specially able toforts, and that were always Isengard of the terrible molten Haunted Misty Mountains had come, on what was he \n",
      "head, they could shape out another by runes in his secrets at least he was very ancient place as practically away, and they named between that running from a loud small men that shall be a \n",
      " waste marshal by vain, yet turned in the sun-rider near with Finwe had escaped \n",
      "trunk. But too it seems well) that finding you had running it. So I will do not not lade what the \n",
      "their sorts of Moria have made \n",
      "utter in their people, as long as they passed in pieces, he came back into a storm has been allowed to betray them to the rear in his sticks. These birthday-hold! So by the Elves without alone, and some that risked the \n",
      "they came is after a song and Traoped up at its land. \n",
      "\n",
      "In Mother left an string and such as yet escaped first Phial of the \n",
      "the place; and as he fell yame from the wall. Appbas with us again, being \n",
      "first Mountain were destroyed after them in file. AND ANDaves to the stars to the borders of the maps, \n",
      "arserually uplands Forest of the dwarves, nor to Br nineness came into the trees, but the voice of the Mountain- fixed \n",
      "the score of such as he and astonished away along the Hill through which Op was listen to the River, which he said to \n",
      "if there were deep which they had dead.’ \n",
      "\n",
      "That came, and the Blueoesls the houses had feeling too changed like a \n",
      "with a admitted upon them and hollow among them on several log loomed away as many sorrow-utter. A hand behind the last fell of horror, screamed by the goblins could track before \n",
      "dragon, and that that looked between the rim of the western fields by the fresh wind beneath Maeglin; nor across their. Forarlum thought they went with foot along \n",
      "Never from the brief gate at last to the western shore. “We might think of \n",
      "tongueience. And too for within it is high-and.” \n",
      "\n",
      "” he said, were after the moment, “and a many eye,” he “and you hadn’em anything of no notice-secret); man, you saw it? \n",
      "’re me,” he thought. A orc-head came, bending out from his following eyes. “We remained in the top of the time to pass dwarves!” \n",
      "\n",
      "The hood of Emy mesh. “Be noon is a kindred!” he said, save is hard to poking his shock as he was \n",
      "come down against them\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dc.fn_decode(model.generate(prompt, max_gen=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b040922-e1c5-4618-8f14-422c0d2790c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4a111-040b-463d-bc53-a11a8c5d5f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
