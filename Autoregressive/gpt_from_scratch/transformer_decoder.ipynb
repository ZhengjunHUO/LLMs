{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198fafb9-477a-4603-a10f-6dcabcf9d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/py3llm/lib/python3.9/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94793c-4e49-4233-a76c-48737fd64cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, path_to_data, ratio_train):\n",
    "        with open(path_to_data, mode='r', encoding='utf-8', errors='replace') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        self.content = content\n",
    "        \n",
    "        # (1) using Home-made solution\n",
    "        # self.vocab = sorted(list(set(content)))\n",
    "        # self.n_vocab = len(self.vocab)\n",
    "        # dict_ctoi = { char:idx for idx, char in enumerate(self.vocab) }\n",
    "        # dict_itoc = { idx:char for idx, char in enumerate(self.vocab) }\n",
    "        # self.fn_encode = lambda s: [dict_ctoi[c] for c in s]\n",
    "        # self.fn_decode = lambda s: ''.join([dict_itoc[i] for i in s])\n",
    "\n",
    "        # (2) using tiktoken\n",
    "        encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.n_vocab = encoding.n_vocab\n",
    "        self.fn_encode = encoding.encode\n",
    "        self.fn_decode = encoding.decode\n",
    "\n",
    "        data = torch.tensor(self.fn_encode(content), dtype=torch.long)\n",
    "        n = int(len(data) * ratio_train)\n",
    "        self.train_data = data[:n]\n",
    "        self.eval_data = data[n:]\n",
    "\n",
    "    def collate_data(self, category, batch_size, context_size):\n",
    "        data = self.train_data if category == 'train' else self.eval_data\n",
    "        batch_start_idx = torch.randint(len(data) - context_size - 1, (batch_size,))\n",
    "        x = torch.stack([data[idx:idx+context_size] for idx in batch_start_idx])\n",
    "        y = torch.stack([data[idx+1:idx+context_size+1] for idx in batch_start_idx])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e33e3fc-f010-4041-a739-5a7e313ee526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in:  3712783\n",
      "50257\n"
     ]
    }
   ],
   "source": [
    "# dc = DataCollator('./TinyS.txt', 0.9)\n",
    "dc = DataCollator('./Tolkien.txt', 0.9)\n",
    "\n",
    "print(\"Read in: \", len(dc.content))\n",
    "print(dc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0130ee-ac7a-4dba-91fb-1872419ab4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, head_size, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.key = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, c, f)\n",
    "        batch, ctx, features = x.shape\n",
    "        # q or k: (b, c, f) @ (f, h) = (b, c, h) where h(head_size) = f / n_head\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        # calc attention score, w: (b, c, c)\n",
    "        w = q @ k.transpose(-2, -1) * features**-0.5\n",
    "        w = w.masked_fill(self.tril[:ctx, :ctx] == 0, float('-inf'))\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        # cal weighted value, v: (b, c, h)\n",
    "        v = self.value(x)\n",
    "        # (b, c, c) @ (b, c, h) = (b, c ,h)\n",
    "        rslt = w @ v\n",
    "        return rslt\n",
    "\n",
    "# params: 4 * n_embedding ^ 2 (Q, K, V, projection)\n",
    "class MaskedMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head\n",
    "        self.heads = torch.nn.ModuleList([MaskedSingleHeadAttention(head_size, context_size, n_embedding, dropout_p) for _ in range(n_head)])\n",
    "        self.projection = torch.nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, c ,h) --cat--> (b, c, f)\n",
    "        rslt = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        rslt = self.dropout(self.projection(rslt))\n",
    "        return rslt\n",
    "\n",
    "# params: 2 * 4 * n_embedding ^ 2\n",
    "class FeedFoward(torch.nn.Module):\n",
    "    def __init__(self, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embedding, n_embedding * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_embedding * 4, n_embedding),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class TransformerUnit(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.mha = MaskedMultiHeadAttention(n_head, context_size, n_embedding, dropout_p)\n",
    "        self.ff = FeedFoward(n_embedding, dropout_p)\n",
    "        self.mha_ln = torch.nn.LayerNorm(n_embedding)\n",
    "        self.ff_ln = torch.nn.LayerNorm(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.mha_ln(x))\n",
    "        x = x + self.ff(self.ff_ln(x))\n",
    "        return x\n",
    "\n",
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding\n",
    "class NaiveLangModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        # params: vocab_size * n_embedding\n",
    "        self.token_embed = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        # params: context_size * n_embedding\n",
    "        self.position_embed = torch.nn.Embedding(context_size, n_embedding)\n",
    "        self.units = torch.nn.Sequential(*[TransformerUnit(n_head, context_size, n_embedding, dropout_p) for _ in range(n_layer)])\n",
    "        self.ln = torch.nn.LayerNorm(n_embedding)\n",
    "        # params: vocab_size * n_embedding\n",
    "        self.pred_head = torch.nn.Linear(n_embedding, vocab_size)\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch, ctx = inputs.shape\n",
    "        # t_embed: (b, c, f); p_embed: (c,f)\n",
    "        t_embed = self.token_embed(inputs)\n",
    "        p_embed = self.position_embed(torch.arange(ctx, device=device))\n",
    "        # x: (b, c, f)\n",
    "        x = t_embed + p_embed\n",
    "        x = self.units(x)\n",
    "        x = self.ln(x)\n",
    "        # logits: (b, c, v) \n",
    "        logits = self.pred_head(x)\n",
    "\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch, ctx, features = logits.shape\n",
    "        predicts = logits.view(batch*ctx, features)\n",
    "        targets = labels.view(batch*ctx)\n",
    "        return logits, torch.nn.functional.cross_entropy(predicts, targets)\n",
    "\n",
    "    def generate(self, inputs, max_gen):\n",
    "        for _ in range(max_gen):\n",
    "            inputs_last_window = inputs[:, -self.context_size:]\n",
    "            logits, loss = self(inputs_last_window)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_next = torch.multinomial(torch.nn.functional.softmax(logits, dim=1), num_samples=1)\n",
    "            inputs = torch.cat((inputs, pred_next), dim=1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961fdd3f-8e0d-4501-8a18-45a9e4e28915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.151057 M parameters\n"
     ]
    }
   ],
   "source": [
    "n_layer = 4\n",
    "#n_head = 4\n",
    "n_head = 6\n",
    "# n_embedding = 256\n",
    "n_embedding = 192\n",
    "dropout_p = 0.2\n",
    "context_size=128 # context length for prediction\n",
    "\n",
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding + 12 * n_embedding ^ 2\n",
    "model = NaiveLangModel(vocab_size=dc.n_vocab, n_layer=n_layer, n_head=n_head, context_size=context_size, n_embedding=n_embedding, dropout_p=dropout_p)\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6f712-a91a-4a51-8991-eeb9d81133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20729c9-0b73-4250-8fa2-29828f2ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, batch_size, steps, eval_interval, n_eval):\n",
    "    @torch.no_grad()\n",
    "    def calc_loss(n_eval, batch_size):\n",
    "        rslt = {}\n",
    "        model.eval()\n",
    "        for c in ['train', 'eval']:\n",
    "            losses = torch.zeros(n_eval)\n",
    "            for i in range(n_eval):\n",
    "                x, y = dc.collate_data(c, batch_size, model.context_size)\n",
    "                _, loss = model(x, y)\n",
    "                losses[i] = loss.item()\n",
    "            rslt[c] = losses.mean()\n",
    "        model.train()\n",
    "        return rslt\n",
    "\n",
    "    def prepare_optimizer(learning_rate, weight_decay):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            for pname, param in module.named_parameters(recurse=False):\n",
    "                full_name = f\"{name}.{pname}\" if name else pname\n",
    "                if pname.endswith(\"bias\") or \"embed\" in name or \"pred_head\" in name:\n",
    "                    no_decay.add(full_name)\n",
    "                else:\n",
    "                    decay.add(full_name)\n",
    "        \n",
    "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "    \n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate)\n",
    "        return optimizer\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    # optimizer = prepare_optimizer(learning_rate=learning_rate, weight_decay=0.01)\n",
    "\n",
    "    for step in range(steps):\n",
    "        if step % eval_interval == 0 or step == steps - 1:\n",
    "            losses = calc_loss(n_eval, batch_size)\n",
    "            print(f\"[step {step}] train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "        x, y = dc.collate_data('train', batch_size, model.context_size)\n",
    "        _, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae67b13a-cd48-4cf0-9cb1-6c409b969a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] train loss 11.0381, eval loss 11.0385\n",
      "[step 100] train loss 5.3590, eval loss 5.4330\n",
      "[step 200] train loss 4.7298, eval loss 4.9026\n",
      "[step 300] train loss 4.4295, eval loss 4.6635\n",
      "[step 400] train loss 4.2294, eval loss 4.5488\n",
      "[step 500] train loss 4.1018, eval loss 4.4709\n",
      "[step 600] train loss 3.9775, eval loss 4.4301\n",
      "[step 700] train loss 3.8573, eval loss 4.3623\n",
      "[step 800] train loss 3.7787, eval loss 4.3467\n",
      "[step 900] train loss 3.6940, eval loss 4.3356\n",
      "[step 1000] train loss 3.6344, eval loss 4.2964\n",
      "[step 1100] train loss 3.5427, eval loss 4.3204\n",
      "[step 1200] train loss 3.4754, eval loss 4.2962\n",
      "[step 1300] train loss 3.4174, eval loss 4.2885\n",
      "[step 1400] train loss 3.3711, eval loss 4.3041\n",
      "[step 1500] train loss 3.3121, eval loss 4.2956\n",
      "[step 1600] train loss 3.2549, eval loss 4.2969\n",
      "[step 1700] train loss 3.2134, eval loss 4.3234\n",
      "[step 1800] train loss 3.1553, eval loss 4.3328\n",
      "[step 1900] train loss 3.1056, eval loss 4.3219\n",
      "[step 2000] train loss 3.0641, eval loss 4.3338\n",
      "[step 2100] train loss 3.0142, eval loss 4.3469\n",
      "[step 2200] train loss 2.9849, eval loss 4.3486\n",
      "[step 2300] train loss 2.9444, eval loss 4.3647\n",
      "[step 2400] train loss 2.9033, eval loss 4.3806\n",
      "[step 2500] train loss 2.8554, eval loss 4.3936\n",
      "[step 2600] train loss 2.8302, eval loss 4.4097\n",
      "[step 2700] train loss 2.7945, eval loss 4.4377\n",
      "[step 2800] train loss 2.7672, eval loss 4.4663\n",
      "[step 2900] train loss 2.7265, eval loss 4.4439\n",
      "[step 3000] train loss 2.7061, eval loss 4.4769\n",
      "[step 3100] train loss 2.6701, eval loss 4.4787\n",
      "[step 3200] train loss 2.6501, eval loss 4.4991\n",
      "[step 3300] train loss 2.6073, eval loss 4.5053\n",
      "[step 3400] train loss 2.5832, eval loss 4.5428\n",
      "[step 3500] train loss 2.5556, eval loss 4.5408\n",
      "[step 3600] train loss 2.5309, eval loss 4.5705\n",
      "[step 3700] train loss 2.5002, eval loss 4.5776\n",
      "[step 3800] train loss 2.4800, eval loss 4.6032\n",
      "[step 3900] train loss 2.4564, eval loss 4.5977\n",
      "[step 4000] train loss 2.4371, eval loss 4.6174\n",
      "[step 4100] train loss 2.4119, eval loss 4.6325\n",
      "[step 4200] train loss 2.3920, eval loss 4.6791\n",
      "[step 4300] train loss 2.3710, eval loss 4.6700\n",
      "[step 4400] train loss 2.3455, eval loss 4.6854\n",
      "[step 4500] train loss 2.3283, eval loss 4.7080\n",
      "[step 4600] train loss 2.3062, eval loss 4.7078\n",
      "[step 4700] train loss 2.2875, eval loss 4.7519\n",
      "[step 4800] train loss 2.2703, eval loss 4.7406\n",
      "[step 4900] train loss 2.2449, eval loss 4.7611\n",
      "[step 4999] train loss 2.2342, eval loss 4.7760\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "eval_interval = 100 # evaluate every N steps\n",
    "# batch_size = 128\n",
    "batch_size = 64\n",
    "n_eval = 100       # evaluate n_eval times then calculate the mean\n",
    "lr = 3e-4\n",
    "\n",
    "train_model(learning_rate=lr, batch_size=batch_size, steps=steps, eval_interval=eval_interval, n_eval=n_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3db2b7-82cf-44fe-b47a-91774bd71887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! I can change your mind away.' At the Lady's shoulder had opened his hand and opened his eyes and vanished. The gloom opened his eyes; slungive at this colour else closed their eyes; then they came to his side:\n",
      "     `Smagol!' cried Shagrat. `Froh, nice water!' A quick end as Gollum had stabbed him in. `Don't wash the rest, go! I want '\n",
      "     `He's no only yet,' he said Sam. `Be up, soft his neck wasn't got more than even an elft as well.'\n",
      "     `Ach! ' His enemy was tied, and as they'll know what they mean. No fear of them'll lose!'\n",
      "     Sam stood out that he fernhelm!' said Sam unled. He was bearing the bundle that he held up and ran, both sterner bolted in all view. 'Well, Lugbrz, eh? But the trick of the magcordor we found here, things needed speed towards Mordor. What about, and why, I had thought to doings made; but at least he made Frodo's getting on the Ring, trying to escape by some good advice, was full. He didn't think that he'd have much _at which he's doing stuck out the true chances.'\n",
      "     Frodo looked at once, especially if to make _ agreement is altogether moved without being half the time for years. That was a copy. It was obviouslyoms anyway.'\n",
      "     The next day was beginning here in his roof and there he slipped out of the room over the stone. It was a desperate mess of all along the sides of the hurling peaks, behind they climbed from the mighty stone at the right; but it was an old stone, picking in its line they started nothing more to keep up their disguise, and then the path both sides of trees, until it passed over these, and were built notched!'\n",
      "     'Not now!' said Merry. 'What do he mean, Pippin, and I tried to show it more about. Only a few gnarled bones-eyes without doings. _Doom,_Ah! I find a copper full, and bow here is where we go and I can't bite in many a hurry. It's looking round a place; and I don't know that room will blow the end of my step.'\n",
      "     'We might mean well enough to be there before this last meal. Not far beyond the mountains can say to Merry looked up towards the Mirror. Just in the West, and lately it will never get quicker than the other reason!'ENTWIFE. Then he thought he burst out aloud in a great danger to catch it.\n",
      "     `A bath well, I've escaped the lightoch from a yard or a struggle that made the rope. The most beloved that fell brought me: my head was less funny, my friends: only asking for a march, Legolas!'\n",
      "       He went off to climb until the hobbits came back to the grass-thrustward side of the Road,\n",
      "     `And if be! ' wailed the dwarves were swinging for him to sleep; but he appeared at this point. He would never get much seem depressed by Legolas when he got up. He sprang lightly and let off the night long knife.\n",
      "     The leaves are fast heaps of leaves.'\n",
      "\n",
      "     Elrond's words were silent, with a deep grow of white gems. `Yrond thought the mad hobbits, I fancy. Orcs bound unwitting built these charming and five passed into the deep! vayed.\n",
      "     `A few others have not touched him,' said Sam.\n",
      "     `And you should have served on the pedestalmers, and Gimli filled the water. Legolas and Gimli was speaking long awake Gimli with his axe-faces grinned high-tongue and far away. Glin and Sam dragged his face.\n",
      "     The night passed together as well as they had done- speech, rejoicingrihyrneek of faded their uplifted roots faded. The rest fled far beyond the Forest the feet of the westward of the hills they found a single glen darkness, for which had wholly dwindled their feet. Where the day there were four fling and dim only hills that of the hills were very hurrying to them, and in build strong places fair, coining abre, slow streams of great unaps of which in the Dark Years-shape end lies. They might have been too great for all; for there will come too great deeds for the friendship to stand by aid to fight against Saur, even if that go over there may be by powers who still shall move alone. But I, and for the board was pressing on the highway that I would not havehelm.'\n",
      "     One day the works of the king had his companions, who had yet been accounted for him, for as a guest of the world in which he read to journey so mounted in Edoras, the Grey is Company for the tidings. How Faramir will return to his father, or I could rule the while that bear men looked to him.'\n",
      "     'He is not his house!' cried Legolas, grimly and clumsy words.'\n",
      "     'A lady,' said Hma, and a strange looks of your hand I have seen my name, and r watching us in their memory, 'and I have failed. Yet my fair lady I spoke of your thought, for perhaps. I have so seldom have I have rather fond me.\n",
      "      'It is the last I said to Merry. 'Then you think that the Ents do not come back, and if it had a legend would I dare to rouse me on withered time. It is all been well, taking me.'\n",
      "     Gandalf did not spoken that Dark Lord had been told long ago in the porch. There was the night when even of Shadowfax reported, or was close far inside the door or the light of the smoke. It was well that they had been silent and beaten in all the watchman, and sweat was lost in greatarguapeless Pass. For a while Sam took them to his party at last to the room there lay on him. At last Frodo turned to his side and ran off the path, and hardly only a to the left.\n",
      "     'We, not and I have passed away,' he said, looking thitherily northwards. 'I can stay in any way up the Withy night I dare no longer.'\n",
      "     Pippin felt his fingers twitch, wondering think, which the had been pleased with the strange strength to it. Here they had come to the end of them, just as a house at the far end, the stronghold of the fields, and it became nearly-clouded, drowned deep, until it could be long lost among the altogether lost, when Faramir denied Samwise Hamfast's guard there is, for of the debate none of Gondor only legend. Of some power seven days am ago, and their strength is yet for will ever grow or their own sure. The Nameless Lord of the Rings again. The Master of him has long delayed a journey from Bucklebury from the Ring, this Fire has been told of old report beyond all his caution, and especially of being given to this Glorfindel and will to guess so.\n",
      "     Frodo felt a moment he crossed his memory, and putting from the Ring to be suspected. Butiquor laughed firmly and turning to the Ring broke the darkness. It glanced to all his mith. He wished Sam that he was looking on his adventure and lying on his face. They lay there as if the Enemy had hold on any other way to escape so long all, and would be their way and Bree, if Sam could find a weak towards the Road.\n",
      "     `Then why didn't they be, Sam, and I can't say? ' they said nobody, turning back to Frodo, borne on the side. `I would be on the top of the way, down on top of the hill.'\n",
      "     `Yes, something the same, or the air is more snow-carrier on,' said Haldir. `What's the Captain sitting down to the middle is all behind,\n",
      "     `No, and we have done it; and you for is the Precious cut to go on our lap of the Rapids, if I am, then it's no tricks has come so back to climb up the stairs. I dares. In the meantime there boughs will be!'\n",
      "     'Meaning I didn't,' said Legolas.\n",
      "      `H shouted Pippin: 'I can't bring him any visible-with to you.' Some of the Halfel. After that Pippin had fewer before. S stretched in the stone door he was cruelly from the dusky roof. Not since the Ent of you arrive, I forget, I think, except that you may keep the yell? You are hidden. And sit on the third I can't begin unfriendly eyes open, unless force as you doubt have shot upon our friends; for I shall not come here.'\n",
      "     Gand\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dc.fn_decode(model.generate(prompt, max_gen=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b040922-e1c5-4618-8f14-422c0d2790c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4a111-040b-463d-bc53-a11a8c5d5f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
