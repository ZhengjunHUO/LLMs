{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198fafb9-477a-4603-a10f-6dcabcf9d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import time\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"[INFO] Using {device} device\")\n",
    "\n",
    "# Acceleration\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94793c-4e49-4233-a76c-48737fd64cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, path_to_data, ratio_train):\n",
    "        with open(path_to_data, mode='r', encoding='utf-8', errors='replace') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        self.content = content\n",
    "        \n",
    "        # (1) using Home-made solution\n",
    "        # self.vocab = sorted(list(set(content)))\n",
    "        # self.n_vocab = len(self.vocab)\n",
    "        # dict_ctoi = { char:idx for idx, char in enumerate(self.vocab) }\n",
    "        # dict_itoc = { idx:char for idx, char in enumerate(self.vocab) }\n",
    "        # self.fn_encode = lambda s: [dict_ctoi[c] for c in s]\n",
    "        # self.fn_decode = lambda s: ''.join([dict_itoc[i] for i in s])\n",
    "\n",
    "        # (2) using tiktoken\n",
    "        encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "        self.n_vocab = encoding.n_vocab\n",
    "        self.fn_encode = encoding.encode\n",
    "        self.fn_decode = encoding.decode\n",
    "\n",
    "        data = torch.tensor(self.fn_encode(content), dtype=torch.long)\n",
    "        n = int(len(data) * ratio_train)\n",
    "        self.train_data = data[:n]\n",
    "        self.eval_data = data[n:]\n",
    "        self.position = 0\n",
    "\n",
    "    def collate_data(self, category, batch_size, context_size):\n",
    "        data = self.train_data if category == 'train' else self.eval_data\n",
    "        batch_start_idx = torch.randint(len(data) - context_size - 1, (batch_size,))\n",
    "        x = torch.stack([data[idx:idx+context_size] for idx in batch_start_idx])\n",
    "        y = torch.stack([data[idx+1:idx+context_size+1] for idx in batch_start_idx])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def next(self, batch_size, context_size):\n",
    "        if self.position + batch_size * context_size + 1 > len(self.train_data):\n",
    "            self.position = 0\n",
    "        batch = self.train_data[self.position: self.position + batch_size * context_size + 1]\n",
    "        x = (batch[:-1]).view(batch_size, context_size)\n",
    "        y = (batch[1:]).view(batch_size, context_size)\n",
    "        self.position += batch_size * context_size\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0130ee-ac7a-4dba-91fb-1872419ab4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, head_size, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.key = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_embedding, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, c, f)\n",
    "        batch, ctx, features = x.shape\n",
    "        # q or k: (b, c, f) @ (f, h) = (b, c, h) where h(head_size) = f / n_head\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        # calc attention score, w: (b, c, c)\n",
    "        w = q @ k.transpose(-2, -1) * q.shape[-1]**-0.5\n",
    "        w = w.masked_fill(self.tril[:ctx, :ctx] == 0, float('-inf'))\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        # cal weighted value, v: (b, c, h)\n",
    "        v = self.value(x)\n",
    "        # (b, c, c) @ (b, c, h) = (b, c ,h)\n",
    "        rslt = w @ v\n",
    "        return rslt\n",
    "\n",
    "# params: 4 * n_embedding ^ 2 (Q, K, V, projection)\n",
    "class MaskedMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        head_size = n_embedding // n_head\n",
    "        self.heads = torch.nn.ModuleList([MaskedSingleHeadAttention(head_size, context_size, n_embedding, dropout_p) for _ in range(n_head)])\n",
    "        self.projection = torch.nn.Linear(n_embedding, n_embedding)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, c ,h) --cat--> (b, c, f)\n",
    "        rslt = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        rslt = self.dropout(self.projection(rslt))\n",
    "        return rslt\n",
    "\n",
    "# params: 2 * 4 * n_embedding ^ 2\n",
    "class FeedFoward(torch.nn.Module):\n",
    "    def __init__(self, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_embedding, n_embedding * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_embedding * 4, n_embedding),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class TransformerUnit(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_embedding, dropout_p):\n",
    "        super().__init__()\n",
    "        self.mha = MaskedMultiHeadAttention(n_head, context_size, n_embedding, dropout_p)\n",
    "        self.ff = FeedFoward(n_embedding, dropout_p)\n",
    "        self.mha_ln = torch.nn.LayerNorm(n_embedding)\n",
    "        self.ff_ln = torch.nn.LayerNorm(n_embedding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.mha_ln(x))\n",
    "        x = x + self.ff(self.ff_ln(x))\n",
    "        return x\n",
    "\n",
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding\n",
    "class NaiveLangModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_head, context_size, n_embedding, dropout_p, use_tie):\n",
    "        super().__init__()\n",
    "        # params: vocab_size * n_embedding\n",
    "        self.token_embed = torch.nn.Embedding(vocab_size, n_embedding)\n",
    "        # params: context_size * n_embedding\n",
    "        self.position_embed = torch.nn.Embedding(context_size, n_embedding)\n",
    "        self.units = torch.nn.Sequential(*[TransformerUnit(n_head, context_size, n_embedding, dropout_p) for _ in range(n_layer)])\n",
    "        self.ln = torch.nn.LayerNorm(n_embedding)\n",
    "        if use_tie:\n",
    "            self.lm_head = torch.nn.Linear(n_embedding, vocab_size, bias=False)\n",
    "            self.lm_head.weight = self.token_embed.weight\n",
    "        else:\n",
    "            # params: vocab_size * n_embedding\n",
    "            self.lm_head = torch.nn.Linear(n_embedding, vocab_size)\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.use_tie = use_tie\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch.nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch, ctx = inputs.shape\n",
    "        # t_embed: (b, c, f); p_embed: (c,f)\n",
    "        t_embed = self.token_embed(inputs)\n",
    "        p_embed = self.position_embed(torch.arange(ctx, device=device))\n",
    "        # x: (b, c, f)\n",
    "        x = t_embed + p_embed\n",
    "        x = self.units(x)\n",
    "        x = self.ln(x)\n",
    "        # logits: (b, c, v) \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch, ctx, features = logits.shape\n",
    "        predicts = logits.view(batch*ctx, features)\n",
    "        targets = labels.view(batch*ctx)\n",
    "        return logits, torch.nn.functional.cross_entropy(predicts, targets)\n",
    "\n",
    "    def generate(self, inputs, max_gen):\n",
    "        for _ in range(max_gen):\n",
    "            inputs_last_window = inputs[:, -self.context_size:]\n",
    "            logits, loss = self(inputs_last_window)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_next = torch.multinomial(torch.nn.functional.softmax(logits, dim=1), num_samples=1)\n",
    "            inputs = torch.cat((inputs, pred_next), dim=1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c20729c9-0b73-4250-8fa2-29828f2ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, batch_size, steps, eval_interval, n_eval, weight_decay, need_compile):\n",
    "    @torch.no_grad()\n",
    "    def calc_loss(n_eval, batch_size):\n",
    "        rslt = {}\n",
    "        model.eval()\n",
    "        for c in ['train', 'eval']:\n",
    "            losses = torch.zeros(n_eval)\n",
    "            for i in range(n_eval):\n",
    "                x, y = dc.collate_data(c, batch_size, model.context_size)\n",
    "                # Acceleration\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    _, loss = model(x, y)\n",
    "                #_, loss = model(x, y)\n",
    "                losses[i] = loss.item()\n",
    "            rslt[c] = losses.mean()\n",
    "        model.train()\n",
    "        return rslt\n",
    "\n",
    "    def prepare_optimizer(learning_rate, weight_decay, need_compile):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        \n",
    "        for name, module in model.named_modules():\n",
    "            for pname, param in module.named_parameters(recurse=False):\n",
    "                full_name = f\"{name}.{pname}\" if name else pname\n",
    "                if pname.endswith(\"bias\") or \"ln\" in name or \"ln\" in pname or \"embed\" in name or \"lm_head\" in name:\n",
    "                    no_decay.add(full_name)\n",
    "                else:\n",
    "                    decay.add(full_name)\n",
    "\n",
    "        # print(\"[decay]: \", decay)\n",
    "        # print(\"[no_decay]: \", no_decay)\n",
    "\n",
    "        if model.use_tie:\n",
    "            if need_compile:\n",
    "                no_decay.remove(\"_orig_mod.lm_head.weight\")\n",
    "            else:\n",
    "                no_decay.remove(\"lm_head.weight\")\n",
    "\n",
    "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "\n",
    "        fused_exist = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        fused = fused_exist and device_type == \"cuda\"\n",
    "        print(f\"[INFO] config AdamW with using fused : {fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=fused)\n",
    "        return optimizer\n",
    "\n",
    "    def update_lr(step):\n",
    "        max_lr = 6e-4\n",
    "        min_lr = max_lr * 0.1\n",
    "        warmup_steps = 100\n",
    "        max_steps = 1000\n",
    "\n",
    "        if step < warmup_steps:\n",
    "            return max_lr * (step+1) / warmup_steps\n",
    "        if step > max_steps:\n",
    "            return min_lr\n",
    "        decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # 1 -> 0\n",
    "        return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    # Apply weight decay\n",
    "    optimizer = prepare_optimizer(learning_rate=learning_rate, weight_decay=weight_decay, need_compile=need_compile)\n",
    "\n",
    "    for step in range(steps):\n",
    "        if step % eval_interval == 0 or step == steps - 1:\n",
    "            losses = calc_loss(n_eval, batch_size)\n",
    "            print(f\"[step {step:4d}] train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        x, y = dc.collate_data('train', batch_size, model.context_size)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # Acceleration\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            _, loss = model(x, y)\n",
    "        #_, loss = model(x, y)\n",
    "        loss.backward()\n",
    "\n",
    "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        lr = update_lr(step)\n",
    "        for param_grp in optimizer.param_groups:\n",
    "            param_grp['lr'] = lr\n",
    "\n",
    "        optimizer.step()\n",
    "        if \"cuda\" in device:\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        throughput = (batch_size * model.context_size) / dt\n",
    "        if step % eval_interval == 0 or step == steps - 1:\n",
    "            print(f\"    elapsed {dt*1000:.2f}ms {throughput:.2f} tok/s norm: {norm:.4f} lr: {lr:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e33e3fc-f010-4041-a739-5a7e313ee526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Read in corpora: 3712783\n",
      "[INFO] Training set tokens: 886222\n",
      "[INFO] Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "dc = DataCollator('./Tolkien.txt', 0.9)\n",
    "\n",
    "print(\"[INFO] Read in corpora:\", len(dc.content))\n",
    "print(\"[INFO] Training set tokens:\", len(dc.train_data))\n",
    "print(\"[INFO] Vocab size:\", dc.n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4b914f-a4f8-48a6-8bea-aa6b4380965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 1 epoch == 54 batches\n"
     ]
    }
   ],
   "source": [
    "# Hyperparam\n",
    "#vocab_size=dc.n_vocab\n",
    "# Acceleration\n",
    "vocab_size=50304\n",
    "n_layer = 4\n",
    "#n_head = 4\n",
    "n_head = 6\n",
    "# n_embedding = 256\n",
    "n_embedding = 384\n",
    "dropout_p = 0.2\n",
    "context_size=256 # context length for prediction\n",
    "\n",
    "steps = 2000\n",
    "eval_interval = 100 # evaluate every N steps\n",
    "# batch_size = 128\n",
    "batch_size = 64\n",
    "n_eval = 100       # evaluate n_eval times then calculate the mean\n",
    "lr = 3e-4\n",
    "#lr = 1e-3\n",
    "weight_decay = 0.01\n",
    "need_compile = True  # sudo apt install python3.9-dev\n",
    "\n",
    "print(f\"[INFO] 1 epoch == {len(dc.train_data) // (batch_size * context_size)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961fdd3f-8e0d-4501-8a18-45a9e4e28915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model params: 26.509056 M parameters\n"
     ]
    }
   ],
   "source": [
    "# params: vocab_size * n_embedding * 2 + context_size * n_embedding + 12 * n_embedding ^ 2\n",
    "model = NaiveLangModel(vocab_size=vocab_size, n_layer=n_layer, n_head=n_head, context_size=context_size, n_embedding=n_embedding, dropout_p=dropout_p, use_tie=True)\n",
    "model = model.to(device)\n",
    "# Acceleration\n",
    "if need_compile:\n",
    "    model = torch.compile(model)\n",
    "print(\"[INFO] Model params:\", sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6f712-a91a-4a51-8991-eeb9d81133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae67b13a-cd48-4cf0-9cb1-6c409b969a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] train loss 10.8630, eval loss 10.8801\n",
      "  elapsed 4890.76ms 3349.99 tok/s\n",
      "[step 100] train loss 5.5445, eval loss 5.8789\n",
      "  elapsed 60.39ms 271283.94 tok/s\n",
      "[step 200] train loss 4.7937, eval loss 5.1919\n",
      "  elapsed 60.48ms 270886.13 tok/s\n",
      "[step 300] train loss 4.4858, eval loss 4.8713\n",
      "  elapsed 60.54ms 270632.23 tok/s\n",
      "[step 400] train loss 4.2457, eval loss 4.7093\n",
      "  elapsed 60.52ms 270740.98 tok/s\n",
      "[step 500] train loss 4.0780, eval loss 4.5750\n",
      "  elapsed 60.68ms 270013.35 tok/s\n",
      "[step 600] train loss 3.9271, eval loss 4.4515\n",
      "  elapsed 60.73ms 269801.33 tok/s\n",
      "[step 700] train loss 3.7877, eval loss 4.3856\n",
      "  elapsed 62.74ms 261122.00 tok/s\n",
      "[step 800] train loss 3.6751, eval loss 4.3421\n",
      "  elapsed 60.99ms 268620.13 tok/s\n",
      "[step 900] train loss 3.5615, eval loss 4.3055\n",
      "  elapsed 60.87ms 269159.87 tok/s\n",
      "[step 1000] train loss 3.4497, eval loss 4.2793\n",
      "  elapsed 64.00ms 255985.18 tok/s\n",
      "[step 1100] train loss 3.3467, eval loss 4.2819\n",
      "  elapsed 60.82ms 269374.06 tok/s\n",
      "[step 1200] train loss 3.2525, eval loss 4.2807\n",
      "  elapsed 65.77ms 249103.65 tok/s\n",
      "[step 1300] train loss 3.1710, eval loss 4.3144\n",
      "  elapsed 60.74ms 269733.55 tok/s\n",
      "[step 1400] train loss 3.0784, eval loss 4.3125\n",
      "  elapsed 60.71ms 269859.60 tok/s\n",
      "[step 1500] train loss 2.9897, eval loss 4.3562\n",
      "  elapsed 61.77ms 265252.43 tok/s\n",
      "[step 1600] train loss 2.9082, eval loss 4.3870\n",
      "  elapsed 61.98ms 264358.55 tok/s\n",
      "[step 1700] train loss 2.8121, eval loss 4.3997\n",
      "  elapsed 60.65ms 270132.22 tok/s\n",
      "[step 1800] train loss 2.7350, eval loss 4.4294\n",
      "  elapsed 60.91ms 268990.25 tok/s\n",
      "[step 1900] train loss 2.6550, eval loss 4.4894\n",
      "  elapsed 60.96ms 268766.16 tok/s\n",
      "[step 1999] train loss 2.5797, eval loss 4.5273\n",
      "  elapsed 62.00ms 264273.15 tok/s\n"
     ]
    }
   ],
   "source": [
    "train_model(learning_rate=lr, batch_size=batch_size, steps=steps, eval_interval=eval_interval, n_eval=n_eval, weight_decay=weight_decay, need_compile=need_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3db2b7-82cf-44fe-b47a-91774bd71887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! Whobe Noldor after all,' he said, 'this will bring you to hers precious Beriand!'\n",
      "     Faramir did not mind both to Hurin as he spoke. 'And what is your gift? For it said no stranger, in the wild men of each of these Riders made them. Is that we stayed long within the road: no sword-hobing without a fleshless thought that I could have found one where it might have long known.'\n",
      "     'And if I would allow them to only to say no wizard to Th oden,' said  owyn, the Warden, in the Lady; and they heard her face. She was shielding their joy then under my cloak. She was Meriadoc, and to his belt, and her guests were her hilt upon the belt. But she spoke with herhood in his stern branching keen hearts; her skill he turned.\n",
      "\n",
      "     Then Sam smiled. `My dear fellows, Mithrandir!' he cried: 'and three powers and plains, in Middle-earth? And strong and wounding themselves blank the Westfolded scent. Already the orc-neys of morning stars snow has be clean.\n",
      "\n",
      "      'I tireless!' he cried. 'You water before the hobbits longer road mass in graveyard in the sky with near the plain. There the sky had be deserted; us it is only a ring of it unsheaterborough in the night. And me see him the earth comes. Alas! But there is an eddyinginker live.'\n",
      "     `No! ' received makilheuard! ' cried Legolas wondering what they could change the foundations of the Mines.\n",
      "     `Strider! ' he said. `My part enemies have dealings with us. If there are many ships, I allow him to the first one that dwell in this land. But my heart, what shall we shall find in years and hunters able to play. We do with you, Gimli, yet shall be\tst. The Wraith-be' eyetell, my heart slept.\n",
      "     `Now we will we help to come to find. Rest an hour and search by the Ruler is early to forbid   following the Bridge, to death, an old solemnly gate, appointed to lose for us.\n",
      "     `It is hidden,' said Balrog. ' \"It knowing our circuit, Gandalf the woodland Uruksanical (with an hour, doom mostly flurry of stone. Beside the tower is flying! Behind meads go thorns down the northern range of the In the tower and ten minutes and look of fire before you saw the mountains. As soon as I came before you had seen them, there was the body of fire, 'helm. I guessed that any coming now we must camp until the last night-d ripples sprang to find a rough stair. Durin's arms trembling hand and the throne rang on the Dike.\n",
      "     'When Bombadil is the value of other roused our way,' he shouted. 'That would be the spare pace, but it was the burning veil of the fine close to the bridge. There is a day, wood and the fall out of Five Armies that perverted to the west side. Had they closed with it up. It seemed, and yet something fair-throated here then made by the no goblin.\n",
      "     ` \"grandfather’ s knowledge could happen before it was so cruel. I thought   if nothing could gather on it seemed to him to me for he could hear it, unless such a brief kitchens, Gbs were our same time in the far cut the Fire; a great black airs or moving now the Great Fell peaks of Dale. And the stars are joining with Uglats.\n",
      "     There seemed close over the valley, and the wilderness Took side as Sam hurled themselves on the right, the fighting tinkling from the ground.\n",
      "\n",
      "     But the silence fell glare in dismay. The hobbits clenched. 'Any Entish destroy the passage!' said Legolas.\n",
      "     Aragorn stood silent again. Legolas looking down at him, with Gimli stood staringted beside the low door, setting far above his shoulder. 'So they come to speak,' he said. 'At last!' And he lifted it up. 'Is there ever old wars of the hills? The riders they are landed and in time! Sit now, and then begin!' Then suddenly Aragorn sat down, and listened slowly a while he seemed to growry the small way and clear, as he caught it struck a few energetic hobbit led a new blue fire out of the entrance to beards over, fine bole it between his head. \"It is almost forgotten in the end of these things,' he said in. 'Where are you going to do? Treebeard? A sign of the floor. Made foruhirither with the top of the gates? Why were crossing the steep banks of the stream/ I led the paths. Black black along the baggage, saved, but they looked down the way beyond Cair A small winding road before, leading them from behind the other height, which of the lake they rested with her short. Over the island hobbits lay there at a great pace, there coloured close for a while.\n",
      "\n",
      "\n",
      "      At length Sam got up for a slow time, until he had been discovered before with any help save for songs of the dangle among the wheels of the mountains. Only a small quarter of all: the tunnel they had become silent; maybe, those that is quiet, alive, having some bite on woods or over there. That night they went round for ever and still bearing him; but perhaps they seemed few fancies, and yet welln decision. They took the way northward for some while ago, they did not see perhaps ask.\n",
      "     'Now,' said Frodo at least stepped over the ledge, and str tunes out round the earth behind him. Some nighed off, wielding their neck.'\n",
      "     'Well shall we come here,' said Frodo. 'But we must rest on our doors of the land, Legolas! There is a Ring too wager.' He stretched uncovered his face.\n",
      "      'It is not so ancient, in these days,' said Aragorn. 'That is quite certain to take true, and so it can perhaps be.'\n",
      "     Then their cloaks they ate, softly took the knife his lips; but Legolas spoke softly: _meas_, _gorg nadan with hoom-beats and coming in this land. _The Ent is said_. Sarn Gollum_Then, or may the Sun be, the head — so far beyond them.'\n",
      "     'Not yet. But we have skins are,' said Frodo, but anyone can do.\n",
      "\n",
      "     `We are worse than that. It is aye, there for some time. But, it is that _is_: the inn is a long years before its own gateag himself. Orcs must die, if not hard to others. But as only the war before Sauron is not overthrown, though Sauron's face. No one will he refuses, Gandalf will do what I am I surely, if I wouldBecause\n",
      "      'Get on him sure-ye,' said Aragorn, 'not yet. A marvel has returned to me when there are none of my doubt. For how there are men die in the Elder Days of a New patrollinghers of yellow hair have never seen again. But now our fear unto choice is so bold, neither taste. True-for these days be cured.'\n",
      "     'Neither did the nassty in tending Bilbo slowed up. 'Soon after nightfall!' cried Gimli, and the wisestere or by filth    flitting should sheolly spring-shpling that round and the view of Osgiliath far away. Darkness goes, and laughing long heave, he gave them even to the fire from his kin, rising from danger: but Osgiliath began again to rise and pass off your home.\n",
      "     Up they came to the forest, rigid as ash: a blow. Then it seemed to the stones broke and flung their prisoners round the ground; andbags, 'No trembled in the stone. all round the sag of the coat of the king. I announced that we shall take our intercies were in the wood. And if we will not forget it only a fire in a mug and    before he had been a holiday.'\n",
      "     'You little meally shall be mistaken; a bit after a drink in fact! You can't wait for your:'\n",
      "      'Galadriel!' said Frodo and Pippin were left in an instant, were gaping. 'I shall not miss the For I am not going to prove the Aragorn to borrow a claim it too hopeful. It is that the ashes.'\n",
      "     'I should I thank you,' said Gimli. 'I am only evil.'\n",
      "     'I shall soon be o' thought only be better,' answered Aragorn. 'He is so great lies before us. Yet he is grown to you, but the one is not the time of its ending. I have accused of it, that Yet\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dc.fn_decode(model.generate(prompt, max_gen=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b040922-e1c5-4618-8f14-422c0d2790c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"[INFO] Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4a111-040b-463d-bc53-a11a8c5d5f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
