{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198fafb9-477a-4603-a10f-6dcabcf9d009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da94793c-4e49-4233-a76c-48737fd64cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    def __init__(self, path_to_data, ratio_train):\n",
    "        with open(path_to_data, mode='r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        self.content = content\n",
    "        self.vocab = sorted(list(set(content)))\n",
    "\n",
    "        dict_ctoi = { char:idx for idx, char in enumerate(self.vocab) }\n",
    "        dict_itoc = { idx:char for idx, char in enumerate(self.vocab) }\n",
    "        self.fn_encode = lambda s: [dict_ctoi[c] for c in s]\n",
    "        self.fn_decode = lambda s: ''.join([dict_itoc[i] for i in s])\n",
    "\n",
    "        data = torch.tensor(self.fn_encode(content), dtype=torch.long)\n",
    "        n = int(len(data) * ratio_train)\n",
    "        self.train_data = data[:n]\n",
    "        self.eval_data = data[n:]\n",
    "\n",
    "    def collate_data(self, category, batch_size, context_size):\n",
    "        data = self.train_data if category == 'train' else self.eval_data\n",
    "        batch_start_idx = torch.randint(len(data) - context_size - 1, (batch_size,))\n",
    "        x = torch.stack([data[idx:idx+context_size] for idx in batch_start_idx])\n",
    "        y = torch.stack([data[idx+1:idx+context_size+1] for idx in batch_start_idx])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e33e3fc-f010-4041-a739-5a7e313ee526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in:  2577900\n",
      "['\\t', '\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 81\n"
     ]
    }
   ],
   "source": [
    "# dc = DataCollator('./TinyS.txt', 0.9)\n",
    "dc = DataCollator('./lotr.txt', 0.9)\n",
    "\n",
    "print(\"Read in: \", len(dc.content))\n",
    "print(dc.vocab, len(dc.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0130ee-ac7a-4dba-91fb-1872419ab4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSingleHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, head_size, context_size, n_feature, dropout_p):\n",
    "        super().__init__()\n",
    "        self.query = torch.nn.Linear(n_feature, head_size, bias=False)\n",
    "        self.key = torch.nn.Linear(n_feature, head_size, bias=False)\n",
    "        self.value = torch.nn.Linear(n_feature, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (b, c, f)\n",
    "        batch, ctx, features = x.shape\n",
    "        # q or k: (b, c, f) @ (f, h) = (b, c, h) where h(head_size) = f / n_head\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        # calc attention score, w: (b, c, c)\n",
    "        w = q @ k.transpose(-2, -1) * features**-0.5\n",
    "        w = w.masked_fill(self.tril[:ctx, :ctx] == 0, float('-inf'))\n",
    "        w = torch.nn.functional.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        # cal weighted value, v: (b, c, h)\n",
    "        v = self.value(x)\n",
    "        # (b, c, c) @ (b, c, h) = (b, c ,h)\n",
    "        rslt = w @ v\n",
    "        return rslt\n",
    "\n",
    "class MaskedMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_feature, dropout_p):\n",
    "        super().__init__()\n",
    "        head_size = n_feature // n_head\n",
    "        self.heads = torch.nn.ModuleList([MaskedSingleHeadAttention(head_size, context_size, n_feature, dropout_p) for _ in range(n_head)])\n",
    "        self.projection = torch.nn.Linear(n_feature, n_feature)\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, c ,h) --cat--> (b, c, f)\n",
    "        rslt = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        rslt = self.dropout(self.projection(rslt))\n",
    "        return rslt\n",
    "\n",
    "class FeedFoward(torch.nn.Module):\n",
    "    def __init__(self, n_feature, dropout_p):\n",
    "        super().__init__()\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_feature, n_feature * 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(n_feature * 4, n_feature),\n",
    "            torch.nn.Dropout(dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "class TransformerUnit(torch.nn.Module):\n",
    "    def __init__(self, n_head, context_size, n_feature, dropout_p):\n",
    "        super().__init__()\n",
    "        self.mha = MaskedMultiHeadAttention(n_head, context_size, n_feature, dropout_p)\n",
    "        self.ff = FeedFoward(n_feature, dropout_p)\n",
    "        self.mha_ln = torch.nn.LayerNorm(n_feature)\n",
    "        self.ff_ln = torch.nn.LayerNorm(n_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.mha(self.mha_ln(x))\n",
    "        x = x + self.ff(self.ff_ln(x))\n",
    "        return x\n",
    "\n",
    "class NaiveLangModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_layer, n_head, context_size, n_feature, dropout_p):\n",
    "        super().__init__()\n",
    "        self.token_embed = torch.nn.Embedding(vocab_size, n_feature)\n",
    "        self.position_embed = torch.nn.Embedding(context_size, n_feature)\n",
    "        self.units = torch.nn.Sequential(*[TransformerUnit(n_head, context_size, n_feature, dropout_p) for _ in range(n_layer)])\n",
    "        self.ln = torch.nn.LayerNorm(n_feature)\n",
    "        self.pred_head = torch.nn.Linear(n_feature, vocab_size)\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch, ctx = inputs.shape\n",
    "        # t_embed: (b, c, f); p_embed: (c,f)\n",
    "        t_embed = self.token_embed(inputs)\n",
    "        p_embed = self.position_embed(torch.arange(ctx, device=device))\n",
    "        # x: (b, c, f)\n",
    "        x = t_embed + p_embed\n",
    "        x = self.units(x)\n",
    "        x = self.ln(x)\n",
    "        # logits: (b, c, v) \n",
    "        logits = self.pred_head(x)\n",
    "\n",
    "        if labels is None:\n",
    "            return logits, None\n",
    "\n",
    "        batch, ctx, features = logits.shape\n",
    "        predicts = logits.view(batch*ctx, features)\n",
    "        targets = labels.view(batch*ctx)\n",
    "        return logits, torch.nn.functional.cross_entropy(predicts, targets)\n",
    "\n",
    "    def generate(self, inputs, max_gen):\n",
    "        for _ in range(max_gen):\n",
    "            inputs_last_window = inputs[:, -self.context_size:]\n",
    "            logits, loss = self(inputs_last_window)\n",
    "            logits = logits[:, -1, :]\n",
    "            pred_next = torch.multinomial(torch.nn.functional.softmax(logits, dim=1), num_samples=1)\n",
    "            inputs = torch.cat((inputs, pred_next), dim=1)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961fdd3f-8e0d-4501-8a18-45a9e4e28915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.263569 M parameters\n"
     ]
    }
   ],
   "source": [
    "n_layer = 4\n",
    "n_head = 4\n",
    "n_feature = 256\n",
    "dropout_p = 0.2\n",
    "context_size=256 # context length for prediction\n",
    "\n",
    "model = NaiveLangModel(vocab_size=len(dc.vocab), n_layer=n_layer, n_head=n_head, context_size=context_size, n_feature=n_feature, dropout_p=dropout_p)\n",
    "model = model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6f712-a91a-4a51-8991-eeb9d81133c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c20729c9-0b73-4250-8fa2-29828f2ac292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(learning_rate, batch_size, steps, eval_interval, n_eval):\n",
    "    @torch.no_grad()\n",
    "    def calc_loss(n_eval, batch_size):\n",
    "        rslt = {}\n",
    "        model.eval()\n",
    "        for c in ['train', 'eval']:\n",
    "            losses = torch.zeros(n_eval)\n",
    "            for i in range(n_eval):\n",
    "                x, y = dc.collate_data(c, batch_size, model.context_size)\n",
    "                _, loss = model(x, y)\n",
    "                losses[i] = loss.item()\n",
    "            rslt[c] = losses.mean()\n",
    "        model.train()\n",
    "        return rslt\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for step in range(steps):\n",
    "        if step % eval_interval == 0 or step == steps - 1:\n",
    "            losses = calc_loss(n_eval, batch_size)\n",
    "            print(f\"[step {step}] train loss {losses['train']:.4f}, eval loss {losses['eval']:.4f}\")\n",
    "    \n",
    "        x, y = dc.collate_data('train', batch_size, model.context_size)\n",
    "        _, loss = model(x, y)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae67b13a-cd48-4cf0-9cb1-6c409b969a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step 0] train loss 4.6559, eval loss 4.6569\n",
      "[step 100] train loss 2.3740, eval loss 2.3711\n",
      "[step 200] train loss 2.2785, eval loss 2.2728\n",
      "[step 300] train loss 1.9580, eval loss 1.9516\n",
      "[step 400] train loss 1.7689, eval loss 1.7617\n",
      "[step 500] train loss 1.6153, eval loss 1.6221\n",
      "[step 600] train loss 1.5218, eval loss 1.5356\n",
      "[step 700] train loss 1.4440, eval loss 1.4649\n",
      "[step 800] train loss 1.3907, eval loss 1.4250\n",
      "[step 900] train loss 1.3522, eval loss 1.3852\n",
      "[step 1000] train loss 1.3144, eval loss 1.3482\n",
      "[step 1100] train loss 1.2914, eval loss 1.3298\n",
      "[step 1200] train loss 1.2688, eval loss 1.3127\n",
      "[step 1300] train loss 1.2451, eval loss 1.2895\n",
      "[step 1400] train loss 1.2293, eval loss 1.2778\n",
      "[step 1500] train loss 1.2135, eval loss 1.2683\n",
      "[step 1600] train loss 1.1973, eval loss 1.2520\n",
      "[step 1700] train loss 1.1868, eval loss 1.2436\n",
      "[step 1800] train loss 1.1770, eval loss 1.2355\n",
      "[step 1900] train loss 1.1720, eval loss 1.2280\n",
      "[step 2000] train loss 1.1576, eval loss 1.2165\n",
      "[step 2100] train loss 1.1480, eval loss 1.2165\n",
      "[step 2200] train loss 1.1420, eval loss 1.2077\n",
      "[step 2300] train loss 1.1381, eval loss 1.2078\n",
      "[step 2400] train loss 1.1285, eval loss 1.1997\n",
      "[step 2500] train loss 1.1236, eval loss 1.1921\n",
      "[step 2600] train loss 1.1184, eval loss 1.1932\n",
      "[step 2700] train loss 1.1115, eval loss 1.1845\n",
      "[step 2800] train loss 1.1047, eval loss 1.1838\n",
      "[step 2900] train loss 1.1031, eval loss 1.1830\n",
      "[step 3000] train loss 1.0940, eval loss 1.1763\n",
      "[step 3100] train loss 1.0925, eval loss 1.1770\n",
      "[step 3200] train loss 1.0901, eval loss 1.1754\n",
      "[step 3300] train loss 1.0840, eval loss 1.1696\n",
      "[step 3400] train loss 1.0831, eval loss 1.1660\n",
      "[step 3500] train loss 1.0774, eval loss 1.1662\n",
      "[step 3600] train loss 1.0737, eval loss 1.1626\n",
      "[step 3700] train loss 1.0697, eval loss 1.1612\n",
      "[step 3800] train loss 1.0691, eval loss 1.1601\n",
      "[step 3900] train loss 1.0622, eval loss 1.1576\n",
      "[step 4000] train loss 1.0603, eval loss 1.1544\n",
      "[step 4100] train loss 1.0593, eval loss 1.1531\n",
      "[step 4200] train loss 1.0549, eval loss 1.1501\n",
      "[step 4300] train loss 1.0508, eval loss 1.1480\n",
      "[step 4400] train loss 1.0505, eval loss 1.1513\n",
      "[step 4500] train loss 1.0453, eval loss 1.1504\n",
      "[step 4600] train loss 1.0430, eval loss 1.1467\n",
      "[step 4700] train loss 1.0427, eval loss 1.1477\n",
      "[step 4800] train loss 1.0377, eval loss 1.1451\n",
      "[step 4900] train loss 1.0356, eval loss 1.1414\n",
      "[step 4999] train loss 1.0341, eval loss 1.1448\n"
     ]
    }
   ],
   "source": [
    "steps = 5000\n",
    "eval_interval = 100 # evaluate every N steps\n",
    "batch_size = 128\n",
    "n_eval = 100       # evaluate n_eval times then calculate the mean\n",
    "lr = 1e-3\n",
    "\n",
    "train_model(learning_rate=lr, batch_size=batch_size, steps=steps, eval_interval=eval_interval, n_eval=n_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba3db2b7-82cf-44fe-b47a-91774bd71887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t fear like the Morgul Vards of Gondor and then, and Gandalf was talking before, along she yet. Between first night, staying that Frodo and left here her becomes, and Gimli Gondor saw him, as sings in a few miles, flying and shrunked it off the childden of Brandywings for the raffor of Old Man, in each of the, the trees of the Dneeth, or at Frodo at leasting ever lureted dringling, turning away. Here and heard amid the remote. It, the men learned with great house that valley, cruel, and over the Ents of Men, and sow on the City stouted about with scatment. great dippearance of her marks were behind. Hidings glanced among the tower. He stooped upon the tread we rose, that he was his beer shewn heralds of Sauron foresed, if all set failes of some stone, and silent birds of Rivendell remotement his picket. Mon's company would come down upon itself, gulling rodly her City then and went up into the lawn, and hung round by his edge.\n",
      "     'Poor. The Councing! The Ninris_ upwards will bring all how indeed. Ever the ring have had been less trainbors accustome by when reluctance. Bill wouldered children audience, and hunger all pierce! I shall not need the Rohirrim father of obycomental of the Deeping Perennor lives; and then Merry will use all his level pressays in the must strong and noble through the fire city. And he, the warness he escaped along the world it is the Lady of the Lords. I will not have at old and since. Before it was beginning a mystre as true you, I will take it a go right; but the Deep has halted it.'\n",
      "     'Then, end Merry is dark save and weary power, maybe, maybe. Well, that is time, you welcome battlement there is many to stead for chins, and here perished in Waiting greatly miles; of his mind had and come all at anger than he came and die, so it read.\n",
      "     'Where we use is not gone, he beging your excitement Treebeard?' No, alone the Enemy fell there was discovice to be a single voice: 'Pearly Nine Nimrodel? I looked at the Ent have of sitial song.\n",
      "     'My cakints, whatever crying horrows of Gondor? Who shal is the Friend the Stond run to our counsel strength unwasde-tale! But we straight with the Shadow, as the Livel made for a deagrous part he walks in the defences. Standir has still only fairs reckoned. But we will be hured for the precious, for he discussed long in the farmhouse. No other sOn the hobbits his ocfits. Gfootped and only hidden, writh he leapt to the south his dark pools. For any pair, no, I never thought I would wait in a great worse, with gleam of a shadow oflow came. And there summer on the roads had been dark, and on the chill voices and valleys, sleer, by nice whine and went. They called up again in, fearing in the sky. They did not lugged amid the western coant, but the tall northward, for with tired treasures they could speak of the grass. Whether was the dead turned southwarden and sparkling oving they ate a great bleak lyined business, walking had runes. Outside against the acrass they dok scraips. Aragorn\n"
     ]
    }
   ],
   "source": [
    "prompt = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(dc.fn_decode(model.generate(prompt, max_gen=3000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b040922-e1c5-4618-8f14-422c0d2790c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4a111-040b-463d-bc53-a11a8c5d5f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
