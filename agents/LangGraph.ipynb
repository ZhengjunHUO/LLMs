{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807899c-d77e-4475-bbc9-443e6a6aeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddb04a5-4745-4cb4-8385-c810f4efcb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "raw_docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs = [doc for raw_doc in raw_docs for doc in raw_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad289419-ed92-488e-85a9-4c05a647b49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap=0)\n",
    "doc_chunks = splitter.split_documents(docs)\n",
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1081af9-790e-4d7a-b106-0f118f4c75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "vec_db = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=OllamaEmbeddings(model=\"qwen2.5-coder:14b\")\n",
    ")\n",
    "retriever = vec_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98824a96-3d19-410d-bfbe-7dfcd7692a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What security issue could we face while using the LLM ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0af242-7a23-4936-b146-827e6096d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ One simple and intuitive way to defend the model against adversarial attacks is \n",
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. H\n",
      "==>  https://lilianweng.github.io/posts/2023-06-23-agent/ }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered\n",
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ In the white-box setting, we have full access to the model parameters and archit\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "relevant_docs = retriever.invoke(question)\n",
    "for relevant_doc in relevant_docs:\n",
    "    print(\"==> \", relevant_doc.metadata['source'], relevant_doc.page_content[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ceadbd-8243-4792-8e7d-90316e166ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f498908c-d36b-439c-8d95-f0988c0d9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckDocs(BaseModel):\n",
    "    is_relevant: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c37bbdd-1b79-4b06-bd03-f26f5c2ae5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5-coder:14b\", temperature=0)\n",
    "llm_structured = llm.with_structured_output(CheckDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49712c9f-d8d8-4041-ad50-ee324cd81bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_instruction = \"\"\"Return only a 'yes' or 'no' depends on whether the document is relevant to the question\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_instruction),\n",
    "        (\"human\", \"Document: \\n {doc} \\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retriv_eval_chain = prompt | llm_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd899fa-a8de-4a1f-9fff-e463e657e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ One simple and intuitive way to defend the model against adversarial attacks is \n",
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. H\n",
      "is_relevant:  is_relevant='no' ==>  https://lilianweng.github.io/posts/2023-06-23-agent/ }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered\n",
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ In the white-box setting, we have full access to the model parameters and archit\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "for relevant_doc in relevant_docs:\n",
    "    content = relevant_doc.page_content\n",
    "    print(\"is_relevant: \", retriv_eval_chain.invoke({\"doc\": content, \"question\": question}), \"==> \", relevant_doc.metadata['source'], relevant_doc.page_content[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b643116-633d-49dc-8a84-96abd2aa089d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb651de5-d556-4b23-b53d-a657bbe4c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e488b33-2b7f-45a9-9914-eb75fac6f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\"\n",
    "),\n",
    "    ]\n",
    ")\n",
    "gen_resp_chain = gen_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a38cb5-84f8-47b5-8e02-3a44afe1e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While using LLMs, one significant security issue is the risk of adversarial attacks, where malicious inputs are crafted to manipulate model outputs. Defending against these attacks by instructing models to avoid harmful content can reduce their success rate but may also lead to decreased general quality and potential misinterpretation of instructions in certain scenarios.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "resp = gen_resp_chain.invoke({\"context\": concat_docs(relevant_docs), \"question\": question})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a2f97-92aa-4632-a347-915abed015e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ea3410-8772-4acb-8e9c-411f025c8940",
   "metadata": {},
   "outputs": [],
   "source": [
    "reform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"\"\"You are a question re-writer that converts an input question to a better version that is optimized for web search. Look at the input and try to reason about the underlying semantic meaning.\"\"\"),\n",
    "        (\"human\", \"This is the original question: {question} \\n Try to rewrite a better one\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reform_question_chain = reform_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23c9c73e-a4b6-4035-9ef6-511beadffbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What security issue could we face while using the LLM ?',\n",
       " 'What are potential security risks associated with using large language models (LLMs)?')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "question, reform_question_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba81c367-4d11-4b92-a41a-f178e4d0d736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f4567-e12b-42ad-862d-e4c806b9fb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e2c01ec-e623-44ec-9e20-96fdf4ee9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    docs: List[str]\n",
    "    question: str\n",
    "    retry: int\n",
    "    gen_without_context: str\n",
    "    resp: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29bf6c35-5745-430f-8dc5-61449986a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_state(state):\n",
    "    print(\"*** INIT STATE ***\")\n",
    "    \n",
    "    return {\"retry\": 3}\n",
    "\n",
    "def retrv_docs(state):\n",
    "    print(\"*** RETRIEVE DOCS ***\")\n",
    "    print(\"  current state: \", state)\n",
    "    \n",
    "    docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"docs\": docs}\n",
    "\n",
    "def eval_docs(state):\n",
    "    print(\"*** CHECK DOCS RELEVANCE ***\")\n",
    "    related_docs = []\n",
    "    gen_without_context = \"FALSE\"\n",
    "\n",
    "    for doc in state[\"docs\"]:\n",
    "        eval_rslt = retriv_eval_chain.invoke({\"doc\": doc.page_content, \"question\": state[\"question\"]})\n",
    "        print(\"  ==> \", doc.metadata['source'], doc.page_content[:80], \" is relevant: \", eval_rslt.is_relevant)\n",
    "        if eval_rslt.is_relevant == \"yes\":\n",
    "            related_docs.append(doc)\n",
    "    if len(related_docs) == 0:\n",
    "        print(\"  !!! NO RELATED DOCS FOUND !!!\")\n",
    "        gen_without_context = \"TRUE\"\n",
    "\n",
    "    return {\"docs\": related_docs, \"gen_without_context\": gen_without_context}\n",
    "\n",
    "def reform_question(state):\n",
    "    print(\"*** REFORM QUESTION ***\")\n",
    "    print(\"  ORIGINAL QUESTION: \", state[\"question\"])\n",
    "\n",
    "    reformed_question = reform_question_chain.invoke({\"question\": state[\"question\"]})\n",
    "    print(\"  REFORMED QUESTION: \", reformed_question)\n",
    "    return {\"question\": reformed_question, \"retry\": state[\"retry\"] - 1}\n",
    "\n",
    "def gen_resp(state):\n",
    "    print(\"*** GENERATE RESPONSE ***\")\n",
    "\n",
    "    resp = gen_resp_chain.invoke({\"context\": state[\"docs\"], \"question\": state[\"question\"]})\n",
    "    return {\"resp\": resp}\n",
    "\n",
    "def retry_or_gen(state):\n",
    "    print(\"*** MAKING DECISION ***\")\n",
    "    if state[\"gen_without_context\"] == \"FALSE\":\n",
    "        print(\"  WILL GENERATE RESPONSE\")\n",
    "        return \"do_gen_resp\"\n",
    "\n",
    "    if state[\"retry\"] == 0:\n",
    "        print(\"  UNABLE TO RETRIEVE DOC, WILL GENERATE RESPONSE\")\n",
    "        return \"do_gen_resp\"\n",
    "\n",
    "    print(\"  TRY TO REFORM THE QUESTION AND RETRIEVE AGAIN\")\n",
    "    return \"do_reform_question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d586761-34e4-4819-b5e3-7cee56bf061d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e7fa1ba-287c-4349-a3b8-f5e57f369220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"init_state\", init_state)\n",
    "graph.add_node(\"retrv_docs\", retrv_docs)\n",
    "graph.add_node(\"eval_docs\", eval_docs)\n",
    "graph.add_node(\"reform_question\", reform_question)\n",
    "graph.add_node(\"gen_resp\", gen_resp)\n",
    "\n",
    "graph.add_edge(START, \"init_state\")\n",
    "graph.add_edge(\"init_state\", \"retrv_docs\")\n",
    "graph.add_edge(\"retrv_docs\", \"eval_docs\")\n",
    "graph.add_conditional_edges(\n",
    "    \"eval_docs\",\n",
    "    retry_or_gen,\n",
    "    {\n",
    "        \"do_gen_resp\": \"gen_resp\",\n",
    "        \"do_reform_question\": \"reform_question\",\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"reform_question\", \"retrv_docs\")\n",
    "graph.add_edge(\"gen_resp\", END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46888601-c1f1-40ae-9842-a8e1cddcb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** INIT STATE ***\n",
      "key:  init_state\n",
      "*** RETRIEVE DOCS ***\n",
      "  current state:  {'question': 'What security issue could we face while using the LLM ?', 'retry': 3}\n",
      "key:  retrv_docs\n",
      "*** CHECK DOCS RELEVANCE ***\n",
      "  ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ One simple and intuitive way to defend the model against adversarial attacks is   is relevant:  yes\n",
      "  ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. H  is relevant:  yes\n",
      "  ==>  https://lilianweng.github.io/posts/2023-06-23-agent/ }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered  is relevant:  no\n",
      "  ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ In the white-box setting, we have full access to the model parameters and archit  is relevant:  yes\n",
      "*** MAKING DECISION ***\n",
      "  WILL GENERATE RESPONSE\n",
      "key:  eval_docs\n",
      "*** GENERATE RESPONSE ***\n",
      "key:  gen_resp\n",
      "While using large language models (LLMs), one significant security issue is the potential for adversarial attacks or jailbreak prompts that could trigger the model to output undesired content. These attacks are challenging due to the discrete nature of text data and the lack of direct gradient signals, making it difficult to control the model's outputs effectively. To mitigate this, explicit instructions can be given to the model to avoid generating harmful content, although this approach may reduce general model quality or lead to incorrect interpretations in certain scenarios.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "user_input = {\"question\": \"What security issue could we face while using the LLM ?\"}\n",
    "for stdout in workflow.stream(user_input):\n",
    "    for key, value in stdout.items():\n",
    "        print(\"key: \", key)\n",
    "        #print(\"value: \", value)\n",
    "\n",
    "print(value[\"resp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8bd1f-07f3-4c82-b6f0-954f3b66f788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
