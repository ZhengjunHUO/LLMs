{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807899c-d77e-4475-bbc9-443e6a6aeb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ddb04a5-4745-4cb4-8385-c810f4efcb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "raw_docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs = [doc for raw_doc in raw_docs for doc in raw_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad289419-ed92-488e-85a9-4c05a647b49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap=0)\n",
    "doc_chunks = splitter.split_documents(docs)\n",
    "len(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1081af9-790e-4d7a-b106-0f118f4c75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "vec_db = Chroma.from_documents(\n",
    "    documents=doc_chunks,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=OllamaEmbeddings(model=\"qwen2.5-coder:14b\")\n",
    ")\n",
    "retriever = vec_db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98824a96-3d19-410d-bfbe-7dfcd7692a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What security issue could we face while using the LLM ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0af242-7a23-4936-b146-827e6096d82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ One simple and intuitive way to defend the model against adversarial attacks is \n",
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. H\n",
      "==>  https://lilianweng.github.io/posts/2023-06-23-agent/ }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered\n",
      "==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ In the white-box setting, we have full access to the model parameters and archit\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "relevant_docs = retriever.invoke(question)\n",
    "for relevant_doc in relevant_docs:\n",
    "    print(\"==> \", relevant_doc.metadata['source'], relevant_doc.page_content[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ceadbd-8243-4792-8e7d-90316e166ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f498908c-d36b-439c-8d95-f0988c0d9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CheckDocs(BaseModel):\n",
    "    is_relevant: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c37bbdd-1b79-4b06-bd03-f26f5c2ae5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5-coder:14b\", temperature=0)\n",
    "llm_structured = llm.with_structured_output(CheckDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49712c9f-d8d8-4041-ad50-ee324cd81bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_instruction = \"\"\"Return only a 'yes' or 'no' depends on whether the document is relevant to the question\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_instruction),\n",
    "        (\"human\", \"Document: \\n {doc} \\n\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "retriv_eval = prompt | llm_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd899fa-a8de-4a1f-9fff-e463e657e8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ One simple and intuitive way to defend the model against adversarial attacks is \n",
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ Fig. 13. UI for humans to do tool-assisted adversarial attack on a classifier. H\n",
      "is_relevant:  is_relevant='no' ==>  https://lilianweng.github.io/posts/2023-06-23-agent/ }\n",
      "]\n",
      "Challenges#\n",
      "After going through key ideas and demos of building LLM-centered\n",
      "is_relevant:  is_relevant='yes' ==>  https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/ In the white-box setting, we have full access to the model parameters and archit\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "for relevant_doc in relevant_docs:\n",
    "    content = relevant_doc.page_content\n",
    "    print(\"is_relevant: \", retriv_eval.invoke({\"doc\": content, \"question\": question}), \"==> \", relevant_doc.metadata['source'], relevant_doc.page_content[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b643116-633d-49dc-8a84-96abd2aa089d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb651de5-d556-4b23-b53d-a657bbe4c3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e488b33-2b7f-45a9-9914-eb75fac6f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\nAnswer:\"\n",
    "),\n",
    "    ]\n",
    ")\n",
    "gen_resp = gen_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6a38cb5-84f8-47b5-8e02-3a44afe1e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While using LLMs, one significant security issue is the risk of adversarial attacks, where malicious inputs are crafted to manipulate model outputs. Defending against these attacks by instructing models to avoid harmful content can reduce their success rate but may also lead to decreased general quality and potential misinterpretation of instructions in certain scenarios.\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "resp = gen_resp.invoke({\"context\": concat_docs(relevant_docs), \"question\": question})\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a2f97-92aa-4632-a347-915abed015e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
